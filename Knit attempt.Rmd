---
Capstone
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
setwd("~/Ryerson Cert/capstone/R code")

install.packages("twitteR")
install.packages("ROAuth")
library("twitteR")
library("ROAuth")
install.packages("wordcloud")
install.packages("tm")
install.packages("xlsx")
# install.packages("XLConnect")
library("wordcloud")
library("tm")
library("xlsx")
# library("XLConnect")
install.packages("rtweet")
library("rtweet")
install.packages("httpuv")
library("httpuv")
install.packages("caret")

library(RTextTools)
library(e1071)
library(dplyr)
library(caret)
install.packages("openxlsx", dependencies = TRUE)
library ("openxlsx")
library(dplyr)


install.packages("~/R/win-library/3.4/stringi_1.2.4.zip", repos=NULL, type = "win.binary")

library(stringr)
install.packages("tidytext")
library(tidytext)

### fetching tweets ###


library("ROAuth")
library("wordcloud")
library("tm")

my_consumer_key <- "p75Cz4TobtdXCNdPONZYqHLoj"
my_consumer_secret <- "YUwK98QpqhhhcfPUppixexln3rdtSOVjLDwLmUatZK01wOnmHb"
my_access_token <- '1044791484812283904-Ej3p6fOAZUbTcRhUMdBU9Anx9MdR4w'
my_access_secret <- 'bfzDZJ93QX0smCGEbDA3ixb1IvkFdmNqWYvCrmsKq4YlJ'




```


# 

```{r}

#--------------------------using rTweet
#create_token(
 # app = "Lesley Milley Text Analytics",
 # consumer_key = my_consumer_key,
 # consumer_secret = my_consumer_secret,access_token = NULL, access_secret = NULL)


search_string <- '@jen_keesmaat OR jenkeesmaat OR johntory OR keesmaat OR #keesmaatformayor OR jen_keesmaat OR #votejohntory OR #keesmaat4mayor OR 
(#topoli OR topoli OR toronto OR toelxn OR #toronto OR #toelxn (#tory OR tory OR jen OR jennifer)) '

no.of.tweets <- 5000
# uncomment line below to collect more tweets
#tweets <- search_tweets(search_string, n=no.of.tweets,  lang="en", type = 'recent')

# import new data
#below run around 5 pm?
#tweetsOct19<- search_tweets(search_string, n=no.of.tweets,  lang="en", type = 'recent')
# below run at 2 pm 
#tweetsOct21<- search_tweets(search_string, n=no.of.tweets,  lang="en", type = 'recent')
# 6 pm on 22nd
#tweetsOct22<- search_tweets(search_string, n=no.of.tweets,  lang="en", type = 'recent')
# at noon on 23
#tweetsOct23<- search_tweets(search_string, n=no.of.tweets,  lang="en", type = 'recent')

# dont touch the two below. these are for saving
#tweetssave<-tweets
#tweetsOct15<-tweets

#saveRDS(tweets, file="Oct15tweets.rds")
# tweets_restore <- readRDS("Oct15tweets.rds")
#saveRDS(tweetsOct19, file="Oct19tweets.rds")
#saveRDS(tweetsOct21, file="Oct21tweets.rds")
#saveRDS(tweetsOct22, file="Oct22tweets.rds")
#saveRDS(tweetsOct23, file="Oct23tweets.rds")

head(tweets)
class(tweets)
#str(tweets)


```




```




```{r}

#openxlsx::write.xlsx(as.data.frame(tweets), file= "Oct15.xlsx", sheetName="Sheet1", row.names=TRUE,  col.names=TRUE, append=FALSE)

# import the annotated tweets this is the rolled up version

annotatedtweet <- read.xlsx("Oct15 edited data to import sheet only one column.xlsx",  1)
table(annotatedtweet$Class)
summary(annotatedtweet)

# import the annotated tweets this maintians the sentiment for john and jen

annotatedtweetfull <- read.xlsx("Oct15 edited data single column all categories.xlsx",  1)
table(annotatedtweetfull$class)
summary(annotatedtweetfull)


annotatedtweetfull2 <- read.xlsx("Oct15 edited data single column all categories v2.xlsx",  1)
table(annotatedtweetfull2$class)
summary(annotatedtweetfull)

#create limited data frame from df "tweets"



fields_to_include <- read.xlsx("Columns.xlsx",  1)
fields_to_include


fields_to_include_with_quotes<-(fields_to_include[['field']])



tweets_most_limited_extended<-tweets[,fields_to_include_with_quotes]


# attach the new columns

# link to annotatedtweetfull
# tweets_for_modelling <- merge(tweets_most_limited,annotatedtweetfull2,by=c("status_id") )
tweets_for_modelling_extended <- merge(tweets_most_limited_extended,annotatedtweetfull2,by=c("status_id") )

# check

# check<-anti_join(tweets_most_limited,annotatedtweetfull2,by=c("status_id") )
# check_reverse<-anti_join(annotatedtweetfull2,tweets_most_limited,by=c("status_id") )

check<-anti_join(tweets_most_limited_extended,annotatedtweetfull2,by=c("status_id") )
check_reverse<-anti_join(annotatedtweetfull2,tweets_most_limited_extended,by=c("status_id") )

# fix the number that excel has truncated - only do once 
# the section below is required as Excel has truncated one of the numbers

tweets_most_limited_extended[1470,"status_id"]
annotatedtweetfull2[1470,"status_id"]
annotatedtweetfull2[1470,"status_id"]<-tweets_most_limited_extended[1470,"status_id"]

check2<-anti_join(tweets_most_limited,annotatedtweetfull2,by=c("status_id") )
check_reverse2<-anti_join(annotatedtweetfull2,tweets_most_limited,by=c("status_id") )

tweets_for_modelling <- merge(tweets_most_limited,annotatedtweetfull2,by=c("status_id") )

```

 


```{r}
# bring in the sentistrength

sentistrength <- read.xlsx("sentiresults.xlsx",  1)

summary(sentistrength)

tweets_for_modelling_extended_ss <- merge(tweets_for_modelling_extended,sentistrength,by=c("status_id") )

check<-anti_join(tweets_for_modelling_extended,sentistrength,by=c("status_id") )
(missing_id<-as.numeric(check$status_id))

tweets_most_limited_extended[1470,"status_id"]
sentistrength[1470,"status_id"]
sentistrength[1470,"status_id"]<-tweets_most_limited_extended[1470,"status_id"]

check_reverse<-anti_join(sentistrength, tweets_for_modelling_extended,by=c("status_id") )
wrong_id<-as.numeric(check_reverse$status_id)

tweets_for_modelling_extended_ss <- merge(tweets_for_modelling_extended,sentistrength,by=c("status_id") )




```

 



```{r}
tweets_book1<-tweets_for_modelling_tidy %>% unnest_tokens(word,text)

afinn<-tweets_book1 %>% inner_join(get_sentiments("afinn")) %>%
  group_by(status_id) %>%
  summarise(afinn_sum =sum(score)) %>%
  mutate(method = "AFINN")

# putting back with the original file

tweets_for_modelling_features <- merge(tweets_for_modelling_extended_ss,afinn,by=c("status_id") ,all.x=TRUE)

# preparing the other features
tweets_for_modelling_features2<-tweets_for_modelling_features

```




```{r}
# adding in a flag for a Toronto location
table(tweets_for_modelling_features2$location)
locationwords<-c("Toronto","Etobicoke","Scarborough", "North York", "East York")


# tweets_for_modelling_features2$Toronto<-ifelse(grepl(locationwords,tweets_for_modelling_features2$location,ignore.case=TRUE),TRUE,FALSE)

tweets_for_modelling_features2$Toronto<-ifelse(grepl(paste(locationwords, collapse = "|"),tweets_for_modelling_features2$location,ignore.case=TRUE),TRUE,FALSE)

table(tweets_for_modelling_features2$location[tweets_for_modelling_features2$Toronto==TRUE])




# flagging if the twitter account was made more than a month before the elecion

str(tweets_for_modelling_features2$account_created_at)

recent_date <- as.POSIXct("2018-08-31 23:59:59")

tweets_for_modelling_features2$new_account<-ifelse(tweets_for_modelling_features2$account_created_at>recent_date,TRUE,FALSE)

# Adding in a flag for mentions

tweets_for_modelling_features2$reply_flag<-ifelse(!is.na(tweets_for_modelling_features2$reply_to_user_id) ,TRUE,FALSE)

tweets_for_modelling_features2$mentions_flag<-ifelse( !is.na(tweets_for_modelling_features2$mentions_user_id) ,TRUE,FALSE)

# adding in a flag if a URL is present

tweets_for_modelling_features2$URL_flag<-ifelse( !is.na(tweets_for_modelling_features2$urls_url) ,TRUE,FALSE)

# limiting the set to relevant features

tweets_for_modelling_features_final<-tweets_for_modelling_features2[,c("Toronto","new_account","reply_flag","mentions_flag","URL_flag","afinn_sum","is_quote","is_retweet","favorite_count","media_type","retweet_count","followers_count","friends_count","statuses_count","verified","class","ss_Positive","ss_Negative")]

```




```{r}
# Modelling Section with Dataset 1 (Features ------------------------------------------------------------------------)

# Split into Test and Training ensuring both are stratified for the categories


# For Training run 10 x 10 Folds for Naive Bayes and Support Vector Machine. For the Training data use stratified folds



# Do a paired comparison 


# repeat above for the Bag of Words data 

```
```{r}


```
```{r}

```

